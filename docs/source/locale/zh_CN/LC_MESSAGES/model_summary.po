# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The Hugging Face Team, Licenced under the Apache
# License, Version 2.0
# This file is distributed under the same license as the transformers
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: transformers \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-30 16:44+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../source/model_summary.rst:14
msgid "Summary of the models"
msgstr ""

#: ../../source/model_summary.rst:16
msgid ""
"This is a summary of the models available in ü§ó Transformers. It assumes "
"you‚Äôre familiar with the original `transformer model "
"<https://arxiv.org/abs/1706.03762>`_. For a gentle introduction check the"
" `annotated transformer "
"<http://nlp.seas.harvard.edu/2018/04/03/attention.html>`_. Here we focus "
"on the high-level differences between the models. You can check them more"
" in detail in their respective documentation. Also check out the "
":doc:`pretrained model page </pretrained_models>` to see the checkpoints "
"available for each type of model and all `the community models "
"<https://huggingface.co/models>`_."
msgstr ""

#: ../../source/model_summary.rst:23
msgid ""
"Each one of the models in the library falls into one of the following "
"categories:"
msgstr ""

#: ../../source/model_summary.rst:25
msgid ":ref:`autoregressive-models`"
msgstr ""

#: ../../source/model_summary.rst:26
msgid ":ref:`autoencoding-models`"
msgstr ""

#: ../../source/model_summary.rst:27
msgid ":ref:`seq-to-seq-models`"
msgstr ""

#: ../../source/model_summary.rst:28
msgid ":ref:`multimodal-models`"
msgstr ""

#: ../../source/model_summary.rst:29
msgid ":ref:`retrieval-based-models`"
msgstr ""

#: ../../source/model_summary.rst:37
msgid ""
"Autoregressive models are pretrained on the classic language modeling "
"task: guess the next token having read all the previous ones. They "
"correspond to the decoder of the original transformer model, and a mask "
"is used on top of the full sentence so that the attention heads can only "
"see what was before in the text, and not what‚Äôs after. Although those "
"models can be fine-tuned and achieve great results on many tasks, the "
"most natural application is text generation. A typical example of such "
"models is GPT."
msgstr ""

#: ../../source/model_summary.rst:43
msgid ""
"Autoencoding models are pretrained by corrupting the input tokens in some"
" way and trying to reconstruct the original sentence. They correspond to "
"the encoder of the original transformer model in the sense that they get "
"access to the full inputs without any mask. Those models usually build a "
"bidirectional representation of the whole sentence. They can be fine-"
"tuned and achieve great results on many tasks such as text generation, "
"but their most natural application is sentence classification or token "
"classification. A typical example of such models is BERT."
msgstr ""

#: ../../source/model_summary.rst:49
msgid ""
"Note that the only difference between autoregressive models and "
"autoencoding models is in the way the model is pretrained. Therefore, the"
" same architecture can be used for both autoregressive and autoencoding "
"models. When a given model has been used for both types of pretraining, "
"we have put it in the category corresponding to the article where it was "
"first introduced."
msgstr ""

#: ../../source/model_summary.rst:54
msgid ""
"Sequence-to-sequence models use both the encoder and the decoder of the "
"original transformer, either for translation tasks or by transforming "
"other tasks to sequence-to-sequence problems. They can be fine-tuned to "
"many tasks but their most natural applications are translation, "
"summarization and question answering. The original transformer model is "
"an example of such a model (only for translation), T5 is an example that "
"can be fine-tuned on other tasks."
msgstr ""

#: ../../source/model_summary.rst:59
msgid ""
"Multimodal models mix text inputs with other kinds (e.g. images) and are "
"more specific to a given task."
msgstr ""

#: ../../source/model_summary.rst:64
msgid "Decoders or autoregressive models"
msgstr ""

#: ../../source/model_summary.rst:66
msgid ""
"As mentioned before, these models rely on the decoder part of the "
"original transformer and use an attention mask so that at each position, "
"the model can only look at the tokens before the attention heads."
msgstr ""

#: ../../source/model_summary.rst:76
msgid "Original GPT"
msgstr ""

#: ../../source/model_summary.rst:87
msgid ""
"`Improving Language Understanding by Generative Pre-Training "
"<https://cdn.openai.com/research-covers/language-"
"unsupervised/language_understanding_paper.pdf>`_, Alec Radford et al."
msgstr ""

#: ../../source/model_summary.rst:90
msgid ""
"The first autoregressive model based on the transformer architecture, "
"pretrained on the Book Corpus dataset."
msgstr ""

#: ../../source/model_summary.rst:92 ../../source/model_summary.rst:114
msgid ""
"The library provides versions of the model for language modeling and "
"multitask language modeling/multiple choice classification."
msgstr ""

#: ../../source/model_summary.rst:96
msgid "GPT-2"
msgstr ""

#: ../../source/model_summary.rst:107
msgid ""
"`Language Models are Unsupervised Multitask Learners "
"<https://d4mucfpksywv.cloudfront.net/better-language-"
"models/language_models_are_unsupervised_multitask_learners.pdf>`_, Alec "
"Radford et al."
msgstr ""

#: ../../source/model_summary.rst:111
msgid ""
"A bigger and better version of GPT, pretrained on WebText (web pages from"
" outgoing links in Reddit with 3 karmas or more)."
msgstr ""

#: ../../source/model_summary.rst:118
msgid "CTRL"
msgstr ""

#: ../../source/model_summary.rst:129
msgid ""
"`CTRL: A Conditional Transformer Language Model for Controllable "
"Generation <https://arxiv.org/abs/1909.05858>`_, Nitish Shirish Keskar et"
" al."
msgstr ""

#: ../../source/model_summary.rst:132
msgid ""
"Same as the GPT model but adds the idea of control codes. Text is "
"generated from a prompt (can be empty) and one (or several) of those "
"control codes which are then used to influence the text generation: "
"generate with the style of wikipedia article, a book or a movie review."
msgstr ""

#: ../../source/model_summary.rst:136 ../../source/model_summary.rst:165
#: ../../source/model_summary.rst:201
msgid "The library provides a version of the model for language modeling only."
msgstr ""

#: ../../source/model_summary.rst:139
msgid "Transformer-XL"
msgstr ""

#: ../../source/model_summary.rst:150
msgid ""
"`Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context "
"<https://arxiv.org/abs/1901.02860>`_, Zihang Dai et al."
msgstr ""

#: ../../source/model_summary.rst:153
msgid ""
"Same as a regular GPT model, but introduces a recurrence mechanism for "
"two consecutive segments (similar to a regular RNNs with two consecutive "
"inputs). In this context, a segment is a number of consecutive tokens "
"(for instance 512) that may span across multiple documents, and segments "
"are fed in order to the model."
msgstr ""

#: ../../source/model_summary.rst:157
msgid ""
"Basically, the hidden states of the previous segment are concatenated to "
"the current input to compute the attention scores. This allows the model "
"to pay attention to information that was in the previous segment as well "
"as the current one. By stacking multiple attention layers, the receptive "
"field can be increased to multiple previous segments."
msgstr ""

#: ../../source/model_summary.rst:161
msgid ""
"This changes the positional embeddings to positional relative embeddings "
"(as the regular positional embeddings would give the same results in the "
"current input and the current hidden state at a given position) and needs"
" to make some adjustments in the way attention scores are computed."
msgstr ""

#: ../../source/model_summary.rst:170
msgid "Reformer"
msgstr ""

#: ../../source/model_summary.rst:181
msgid ""
"`Reformer: The Efficient Transformer "
"<https://arxiv.org/abs/2001.04451>`_, Nikita Kitaev et al ."
msgstr ""

#: ../../source/model_summary.rst:183
msgid ""
"An autoregressive transformer model with lots of tricks to reduce memory "
"footprint and compute time. Those tricks include:"
msgstr ""

#: ../../source/model_summary.rst:186
msgid ""
"Use :ref:`Axial position encoding <axial-pos-encoding>` (see below for "
"more details). It‚Äôs a mechanism to avoid having a huge positional "
"encoding matrix (when the sequence length is very big) by factorizing it "
"into smaller matrices."
msgstr ""

#: ../../source/model_summary.rst:189
msgid ""
"Replace traditional attention by :ref:`LSH (local-sensitive hashing) "
"attention <lsh-attention>` (see below for more details). It's a technique"
" to avoid computing the full product query-key in the attention layers."
msgstr ""

#: ../../source/model_summary.rst:191
msgid ""
"Avoid storing the intermediate results of each layer by using reversible "
"transformer layers to obtain them during the backward pass (subtracting "
"the residuals from the input of the next layer gives them back) or "
"recomputing them for results inside a given layer (less efficient than "
"storing them but saves memory)."
msgstr ""

#: ../../source/model_summary.rst:194
msgid "Compute the feedforward operations by chunks and not on the whole batch."
msgstr ""

#: ../../source/model_summary.rst:196
msgid ""
"With those tricks, the model can be fed much larger sentences than "
"traditional transformer autoregressive models."
msgstr ""

#: ../../source/model_summary.rst:198
msgid ""
"**Note:** This model could be very well be used in an autoencoding "
"setting, there is no checkpoint for such a pretraining yet, though."
msgstr ""

#: ../../source/model_summary.rst:204
msgid "XLNet"
msgstr ""

#: ../../source/model_summary.rst:215
msgid ""
"`XLNet: Generalized Autoregressive Pretraining for Language Understanding"
" <https://arxiv.org/abs/1906.08237>`_, Zhilin Yang et al."
msgstr ""

#: ../../source/model_summary.rst:218
msgid ""
"XLNet is not a traditional autoregressive model but uses a training "
"strategy that builds on that. It permutes the tokens in the sentence, "
"then allows the model to use the last n tokens to predict the token n+1. "
"Since this is all done with a mask, the sentence is actually fed in the "
"model in the right order, but instead of masking the first n tokens for "
"n+1, XLNet uses a mask that hides the previous tokens in some given "
"permutation of 1,...,sequence length."
msgstr ""

#: ../../source/model_summary.rst:223
msgid ""
"XLNet also uses the same recurrence mechanism as Transformer-XL to build "
"long-term dependencies."
msgstr ""

#: ../../source/model_summary.rst:225
msgid ""
"The library provides a version of the model for language modeling, token "
"classification, sentence classification, multiple choice classification "
"and question answering."
msgstr ""

#: ../../source/model_summary.rst:231
msgid "Encoders or autoencoding models"
msgstr ""

#: ../../source/model_summary.rst:233
msgid ""
"As mentioned before, these models rely on the encoder part of the "
"original transformer and use no mask so the model can look at all the "
"tokens in the attention heads. For pretraining, targets are the original "
"sentences and inputs are their corrupted versions."
msgstr ""

#: ../../source/model_summary.rst:244
msgid "BERT"
msgstr ""

#: ../../source/model_summary.rst:255
msgid ""
"`BERT: Pre-training of Deep Bidirectional Transformers for Language "
"Understanding <https://arxiv.org/abs/1810.04805>`_, Jacob Devlin et al."
msgstr ""

#: ../../source/model_summary.rst:258
msgid ""
"Corrupts the inputs by using random masking, more precisely, during "
"pretraining, a given percentage of tokens (usually 15%) is masked by:"
msgstr ""

#: ../../source/model_summary.rst:261
msgid "a special mask token with probability 0.8"
msgstr ""

#: ../../source/model_summary.rst:262
msgid "a random token different from the one masked with probability 0.1"
msgstr ""

#: ../../source/model_summary.rst:263
msgid "the same token with probability 0.1"
msgstr ""

#: ../../source/model_summary.rst:265
msgid ""
"The model must predict the original sentence, but has a second objective:"
" inputs are two sentences A and B (with a separation token in between). "
"With probability 50%, the sentences are consecutive in the corpus, in the"
" remaining 50% they are not related. The model has to predict if the "
"sentences are consecutive or not."
msgstr ""

#: ../../source/model_summary.rst:269
msgid ""
"The library provides a version of the model for language modeling "
"(traditional or masked), next sentence prediction, token classification, "
"sentence classification, multiple choice classification and question "
"answering."
msgstr ""

#: ../../source/model_summary.rst:273
msgid "ALBERT"
msgstr ""

#: ../../source/model_summary.rst:284
msgid ""
"`ALBERT: A Lite BERT for Self-supervised Learning of Language "
"Representations <https://arxiv.org/abs/1909.11942>`_, Zhenzhong Lan et "
"al."
msgstr ""

#: ../../source/model_summary.rst:287
msgid "Same as BERT but with a few tweaks:"
msgstr ""

#: ../../source/model_summary.rst:289
msgid ""
"Embedding size E is different from hidden size H justified because the "
"embeddings are context independent (one embedding vector represents one "
"token), whereas hidden states are context dependent (one hidden state "
"represents a sequence of tokens) so it's more logical to have H >> E. "
"Also, the embedding matrix is large since it's V x E (V being the vocab "
"size). If E < H, it has less parameters."
msgstr ""

#: ../../source/model_summary.rst:293
msgid "Layers are split in groups that share parameters (to save memory)."
msgstr ""

#: ../../source/model_summary.rst:294
msgid ""
"Next sentence prediction is replaced by a sentence ordering prediction: "
"in the inputs, we have two sentences A and B (that are consecutive) and "
"we either feed A followed by B or B followed by A. The model must predict"
" if they have been swapped or not."
msgstr ""

#: ../../source/model_summary.rst:298 ../../source/model_summary.rst:323
#: ../../source/model_summary.rst:434 ../../source/model_summary.rst:508
#: ../../source/model_summary.rst:537
msgid ""
"The library provides a version of the model for masked language modeling,"
" token classification, sentence classification, multiple choice "
"classification and question answering."
msgstr ""

#: ../../source/model_summary.rst:302
msgid "RoBERTa"
msgstr ""

#: ../../source/model_summary.rst:313
msgid ""
"`RoBERTa: A Robustly Optimized BERT Pretraining Approach "
"<https://arxiv.org/abs/1907.11692>`_, Yinhan Liu et al."
msgstr ""

#: ../../source/model_summary.rst:315
msgid "Same as BERT with better pretraining tricks:"
msgstr ""

#: ../../source/model_summary.rst:317
msgid ""
"dynamic masking: tokens are masked differently at each epoch, whereas "
"BERT does it once and for all"
msgstr ""

#: ../../source/model_summary.rst:318
msgid ""
"no NSP (next sentence prediction) loss and instead of putting just two "
"sentences together, put a chunk of contiguous texts together to reach 512"
" tokens (so the sentences are in an order than may span several "
"documents)"
msgstr ""

#: ../../source/model_summary.rst:320
msgid "train with larger batches"
msgstr ""

#: ../../source/model_summary.rst:321
msgid ""
"use BPE with bytes as a subunit and not characters (because of unicode "
"characters)"
msgstr ""

#: ../../source/model_summary.rst:327
msgid "DistilBERT"
msgstr ""

#: ../../source/model_summary.rst:338
msgid ""
"`DistilBERT, a distilled version of BERT: smaller, faster, cheaper and "
"lighter <https://arxiv.org/abs/1910.01108>`_, Victor Sanh et al."
msgstr ""

#: ../../source/model_summary.rst:341
msgid ""
"Same as BERT but smaller. Trained by distillation of the pretrained BERT "
"model, meaning it's been trained to predict the same probabilities as the"
" larger model. The actual objective is a combination of:"
msgstr ""

#: ../../source/model_summary.rst:344
msgid "finding the same probabilities as the teacher model"
msgstr ""

#: ../../source/model_summary.rst:345
msgid "predicting the masked tokens correctly (but no next-sentence objective)"
msgstr ""

#: ../../source/model_summary.rst:346
msgid ""
"a cosine similarity between the hidden states of the student and the "
"teacher model"
msgstr ""

#: ../../source/model_summary.rst:348 ../../source/model_summary.rst:378
msgid ""
"The library provides a version of the model for masked language modeling,"
" token classification, sentence classification and question answering."
msgstr ""

#: ../../source/model_summary.rst:352
msgid "ConvBERT"
msgstr ""

#: ../../source/model_summary.rst:363
msgid ""
"`ConvBERT: Improving BERT with Span-based Dynamic Convolution "
"<https://arxiv.org/abs/1910.01108>`_, Zihang Jiang, Weihao Yu, Daquan "
"Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan."
msgstr ""

#: ../../source/model_summary.rst:366
msgid ""
"Pre-trained language models like BERT and its variants have recently "
"achieved impressive performance in various natural language understanding"
" tasks. However, BERT heavily relies on the global self-attention block "
"and thus suffers large memory footprint and computation cost. Although "
"all its attention heads query on the whole input sequence for generating "
"the attention map from a global perspective, we observe some heads only "
"need to learn local dependencies, which means the existence of "
"computation redundancy. We therefore propose a novel span-based dynamic "
"convolution to replace these self-attention heads to directly model local"
" dependencies. The novel convolution heads, together with the rest self-"
"attention heads, form a new mixed attention block that is more efficient "
"at both global and local context learning. We equip BERT with this mixed "
"attention design and build a ConvBERT model. Experiments have shown that "
"ConvBERT significantly outperforms BERT and its variants in various "
"downstream tasks, with lower training cost and fewer model parameters. "
"Remarkably, ConvBERTbase model achieves 86.4 GLUE score, 0.7 higher than "
"ELECTRAbase, while using less than 1/4 training cost."
msgstr ""

#: ../../source/model_summary.rst:382
msgid "XLM"
msgstr ""

#: ../../source/model_summary.rst:393
msgid ""
"`Cross-lingual Language Model Pretraining "
"<https://arxiv.org/abs/1901.07291>`_, Guillaume Lample and Alexis Conneau"
msgstr ""

#: ../../source/model_summary.rst:395
msgid ""
"A transformer model trained on several languages. There are three "
"different type of training for this model and the library provides "
"checkpoints for all of them:"
msgstr ""

#: ../../source/model_summary.rst:398
msgid ""
"Causal language modeling (CLM) which is the traditional autoregressive "
"training (so this model could be in the previous section as well). One of"
" the languages is selected for each training sample, and the model input "
"is a sentence of 256 tokens, that may span over several documents in one "
"of those languages."
msgstr ""

#: ../../source/model_summary.rst:401
msgid ""
"Masked language modeling (MLM) which is like RoBERTa. One of the "
"languages is selected for each training sample, and the model input is a "
"sentence of 256 tokens, that may span over several documents in one of "
"those languages, with dynamic masking of the tokens."
msgstr ""

#: ../../source/model_summary.rst:404
msgid ""
"A combination of MLM and translation language modeling (TLM). This "
"consists of concatenating a sentence in two different languages, with "
"random masking. To predict one of the masked tokens, the model can use "
"both, the surrounding context in language 1 and the context given by "
"language 2."
msgstr ""

#: ../../source/model_summary.rst:408
msgid ""
"Checkpoints refer to which method was used for pretraining by having "
"`clm`, `mlm` or `mlm-tlm` in their names. On top of positional "
"embeddings, the model has language embeddings. When training using "
"MLM/CLM, this gives the model an indication of the language used, and "
"when training using MLM+TLM, an indication of the language used for each "
"part."
msgstr ""

#: ../../source/model_summary.rst:412
msgid ""
"The library provides a version of the model for language modeling, token "
"classification, sentence classification and question answering."
msgstr ""

#: ../../source/model_summary.rst:416
msgid "XLM-RoBERTa"
msgstr ""

#: ../../source/model_summary.rst:427
msgid ""
"`Unsupervised Cross-lingual Representation Learning at Scale "
"<https://arxiv.org/abs/1911.02116>`_, Alexis Conneau et al."
msgstr ""

#: ../../source/model_summary.rst:430
msgid ""
"Uses RoBERTa tricks on the XLM approach, but does not use the translation"
" language modeling objective. It only uses masked language modeling on "
"sentences coming from one language. However, the model is trained on many"
" more languages (100) and doesn't use the language embeddings, so it's "
"capable of detecting the input language by itself."
msgstr ""

#: ../../source/model_summary.rst:438
msgid "FlauBERT"
msgstr ""

#: ../../source/model_summary.rst:449
msgid ""
"`FlauBERT: Unsupervised Language Model Pre-training for French "
"<https://arxiv.org/abs/1912.05372>`_, Hang Le et al."
msgstr ""

#: ../../source/model_summary.rst:451
msgid ""
"Like RoBERTa, without the sentence ordering prediction (so just trained "
"on the MLM objective)."
msgstr ""

#: ../../source/model_summary.rst:453
msgid ""
"The library provides a version of the model for language modeling and "
"sentence classification."
msgstr ""

#: ../../source/model_summary.rst:456
msgid "ELECTRA"
msgstr ""

#: ../../source/model_summary.rst:467
msgid ""
"`ELECTRA: Pre-training Text Encoders as Discriminators Rather Than "
"Generators <https://arxiv.org/abs/2003.10555>`_, Kevin Clark et al."
msgstr ""

#: ../../source/model_summary.rst:470
msgid ""
"ELECTRA is a transformer model pretrained with the use of another (small)"
" masked language model. The inputs are corrupted by that language model, "
"which takes an input text that is randomly masked and outputs a text in "
"which ELECTRA has to predict which token is an original and which one has"
" been replaced. Like for GAN training, the small language model is "
"trained for a few steps (but with the original texts as objective, not to"
" fool the ELECTRA model like in a traditional GAN setting) then the "
"ELECTRA model is trained for a few steps."
msgstr ""

#: ../../source/model_summary.rst:476
msgid ""
"The library provides a version of the model for masked language modeling,"
" token classification and sentence classification."
msgstr ""

#: ../../source/model_summary.rst:480
msgid "Funnel Transformer"
msgstr ""

#: ../../source/model_summary.rst:491
msgid ""
"`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient "
"Language Processing <https://arxiv.org/abs/2006.03236>`_, Zihang Dai et "
"al."
msgstr ""

#: ../../source/model_summary.rst:494
msgid ""
"Funnel Transformer is a transformer model using pooling, a bit like a "
"ResNet model: layers are grouped in blocks, and at the beginning of each "
"block (except the first one), the hidden states are pooled among the "
"sequence dimension. This way, their length is divided by 2, which speeds "
"up the computation of the next hidden states. All pretrained models have "
"three blocks, which means the final hidden state has a sequence length "
"that is one fourth of the original sequence length."
msgstr ""

#: ../../source/model_summary.rst:500
msgid ""
"For tasks such as classification, this is not a problem, but for tasks "
"like masked language modeling or token classification, we need a hidden "
"state with the same sequence length as the original input. In those "
"cases, the final hidden states are upsampled to the input sequence length"
" and go through two additional layers. That's why there are two versions "
"of each checkpoint. The version suffixed with \"-base\" contains only the"
" three blocks, while the version without that suffix contains the three "
"blocks and the upsampling head with its additional layers."
msgstr ""

#: ../../source/model_summary.rst:506
msgid ""
"The pretrained models available use the same pretraining objective as "
"ELECTRA."
msgstr ""

#: ../../source/model_summary.rst:514
msgid "Longformer"
msgstr ""

#: ../../source/model_summary.rst:525
msgid ""
"`Longformer: The Long-Document Transformer "
"<https://arxiv.org/abs/2004.05150>`_, Iz Beltagy et al."
msgstr ""

#: ../../source/model_summary.rst:527
msgid ""
"A transformer model replacing the attention matrices by sparse matrices "
"to go faster. Often, the local context (e.g., what are the two tokens "
"left and right?) is enough to take action for a given token. Some "
"preselected input tokens are still given global attention, but the "
"attention matrix has way less parameters, resulting in a speed-up. See "
"the :ref:`local attention section <local-attention>` for more "
"information."
msgstr ""

#: ../../source/model_summary.rst:532
msgid "It is pretrained the same way a RoBERTa otherwise."
msgstr ""

#: ../../source/model_summary.rst:534
msgid ""
"**Note:** This model could be very well be used in an autoregressive "
"setting, there is no checkpoint for such a pretraining yet, though."
msgstr ""

#: ../../source/model_summary.rst:543
msgid "Sequence-to-sequence models"
msgstr ""

#: ../../source/model_summary.rst:545
msgid ""
"As mentioned before, these models keep both the encoder and the decoder "
"of the original transformer."
msgstr ""

#: ../../source/model_summary.rst:554
msgid "BART"
msgstr ""

#: ../../source/model_summary.rst:565
msgid ""
"`BART: Denoising Sequence-to-Sequence Pre-training for Natural Language "
"Generation, Translation, and Comprehension "
"<https://arxiv.org/abs/1910.13461>`_, Mike Lewis et al."
msgstr ""

#: ../../source/model_summary.rst:568
msgid ""
"Sequence-to-sequence model with an encoder and a decoder. Encoder is fed "
"a corrupted version of the tokens, decoder is fed the original tokens "
"(but has a mask to hide the future words like a regular transformers "
"decoder). A composition of the following transformations are applied on "
"the pretraining tasks for the encoder:"
msgstr ""

#: ../../source/model_summary.rst:572
msgid "mask random tokens (like in BERT)"
msgstr ""

#: ../../source/model_summary.rst:573
msgid "delete random tokens"
msgstr ""

#: ../../source/model_summary.rst:574
msgid ""
"mask a span of k tokens with a single mask token (a span of 0 tokens is "
"an insertion of a mask token)"
msgstr ""

#: ../../source/model_summary.rst:575
msgid "permute sentences"
msgstr ""

#: ../../source/model_summary.rst:576
msgid "rotate the document to make it start at a specific token"
msgstr ""

#: ../../source/model_summary.rst:578
msgid ""
"The library provides a version of this model for conditional generation "
"and sequence classification."
msgstr ""

#: ../../source/model_summary.rst:581
msgid "Pegasus"
msgstr ""

#: ../../source/model_summary.rst:592
msgid ""
"`PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive "
"Summarization <https://arxiv.org/pdf/1912.08777.pdf>`_, Jingqing Zhang, "
"Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019."
msgstr ""

#: ../../source/model_summary.rst:595
msgid ""
"Sequence-to-sequence model with the same encoder-decoder model "
"architecture as BART. Pegasus is pre-trained jointly on two self-"
"supervised objective functions: Masked Language Modeling (MLM) and a "
"novel summarization specific pretraining objective, called Gap Sentence "
"Generation (GSG)."
msgstr ""

#: ../../source/model_summary.rst:599
msgid ""
"MLM: encoder input tokens are randomly replaced by a mask tokens and have"
" to be predicted by the encoder (like in BERT)"
msgstr ""

#: ../../source/model_summary.rst:601
msgid ""
"GSG: whole encoder input sentences are replaced by a second mask token "
"and fed to the decoder, but which has a causal mask to hide the future "
"words like a regular auto-regressive transformer decoder."
msgstr ""

#: ../../source/model_summary.rst:604
msgid ""
"In contrast to BART, Pegasus' pretraining task is intentionally similar "
"to summarization: important sentences are masked and are generated "
"together as one output sequence from the remaining sentences, similar to "
"an extractive summary."
msgstr ""

#: ../../source/model_summary.rst:608
msgid ""
"The library provides a version of this model for conditional generation, "
"which should be used for summarization."
msgstr ""

#: ../../source/model_summary.rst:612
msgid "MarianMT"
msgstr ""

#: ../../source/model_summary.rst:623
msgid ""
"`Marian: Fast Neural Machine Translation in C++ "
"<https://arxiv.org/abs/1804.00344>`_, Marcin Junczys-Dowmunt et al."
msgstr ""

#: ../../source/model_summary.rst:625
msgid "A framework for translation models, using the same models as BART"
msgstr ""

#: ../../source/model_summary.rst:627 ../../source/model_summary.rst:660
#: ../../source/model_summary.rst:681 ../../source/model_summary.rst:703
msgid "The library provides a version of this model for conditional generation."
msgstr ""

#: ../../source/model_summary.rst:631
msgid "T5"
msgstr ""

#: ../../source/model_summary.rst:642
msgid ""
"`Exploring the Limits of Transfer Learning with a Unified Text-to-Text "
"Transformer <https://arxiv.org/abs/1910.10683>`_, Colin Raffel et al."
msgstr ""

#: ../../source/model_summary.rst:645
msgid ""
"Uses the traditional transformer model (with a slight change in the "
"positional embeddings, which are learned at each layer). To be able to "
"operate on all NLP tasks, it transforms them into text-to-text problems "
"by using specific prefixes: ‚Äúsummarize: ‚Äù, ‚Äúquestion: ‚Äù, ‚Äútranslate "
"English to German: ‚Äù and so forth."
msgstr ""

#: ../../source/model_summary.rst:649
msgid ""
"The pretraining includes both supervised and self-supervised training. "
"Supervised training is conducted on downstream tasks provided by the GLUE"
" and SuperGLUE benchmarks (converting them into text-to-text tasks as "
"explained above)."
msgstr ""

#: ../../source/model_summary.rst:652
#, python-format
msgid ""
"Self-supervised training uses corrupted tokens, by randomly removing 15% "
"of the tokens and replacing them with individual sentinel tokens (if "
"several consecutive tokens are marked for removal, the whole group is "
"replaced with a single sentinel token). The input of the encoder is the "
"corrupted sentence, the input of the decoder is the original sentence and"
" the target is then the dropped out tokens delimited by their sentinel "
"tokens."
msgstr ""

#: ../../source/model_summary.rst:657
msgid ""
"For instance, if we have the sentence ‚ÄúMy dog is very cute .‚Äù, and we "
"decide to remove the tokens: \"dog\", \"is\" and \"cute\", the encoder "
"input becomes ‚ÄúMy <x> very <y> .‚Äù and the target input becomes ‚Äú<x> dog "
"is <y> cute .<z>‚Äù"
msgstr ""

#: ../../source/model_summary.rst:664
msgid "MT5"
msgstr ""

#: ../../source/model_summary.rst:675
msgid ""
"`mT5: A massively multilingual pre-trained text-to-text transformer "
"<https://arxiv.org/abs/2010.11934>`_, Linting Xue et al."
msgstr ""

#: ../../source/model_summary.rst:678
msgid ""
"The model architecture is same as T5. mT5's pretraining objective "
"includes T5's self-supervised training, but not T5's supervised training."
" mT5 is trained on 101 languages."
msgstr ""

#: ../../source/model_summary.rst:685
msgid "MBart"
msgstr ""

#: ../../source/model_summary.rst:696
msgid ""
"`Multilingual Denoising Pre-training for Neural Machine Translation "
"<https://arxiv.org/abs/2001.08210>`_ by Yinhan Liu, Jiatao Gu, Naman "
"Goyal, Xian Li, Sergey Edunov Marjan Ghazvininejad, Mike Lewis, Luke "
"Zettlemoyer."
msgstr ""

#: ../../source/model_summary.rst:699
msgid ""
"The model architecture and pretraining objective is same as BART, but "
"MBart is trained on 25 languages and is intended for supervised and "
"unsupervised machine translation. MBart is one of the first methods for "
"pretraining a complete sequence-to-sequence model by denoising full texts"
" in multiple languages,"
msgstr ""

#: ../../source/model_summary.rst:705
msgid ""
"The `mbart-large-en-ro checkpoint <https://huggingface.co/facebook/mbart-"
"large-en-ro>`_ can be used for english -> romanian translation."
msgstr ""

#: ../../source/model_summary.rst:708
msgid ""
"The `mbart-large-cc25 <https://huggingface.co/facebook/mbart-large-"
"cc25>`_ checkpoint can be finetuned for other translation and "
"summarization tasks, using code in ```examples/pytorch/translation/``` , "
"but is not very useful without finetuning."
msgstr ""

#: ../../source/model_summary.rst:714
msgid "ProphetNet"
msgstr ""

#: ../../source/model_summary.rst:725 ../../source/model_summary.rst:750
msgid ""
"`ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-"
"training, <https://arxiv.org/abs/2001.04063>`__ by Yu Yan, Weizhen Qi, "
"Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, Ming "
"Zhou."
msgstr ""

#: ../../source/model_summary.rst:728
msgid ""
"ProphetNet introduces a novel *sequence-to-sequence* pretraining "
"objective, called *future n-gram prediction*. In future n-gram "
"prediction, the model predicts the next n tokens simultaneously based on "
"previous context tokens at each time step instead instead of just the "
"single next token. The future n-gram prediction explicitly encourages the"
" model to plan for the future tokens and prevent overfitting on strong "
"local correlations. The model architecture is based on the original "
"Transformer, but replaces the \"standard\" self-attention mechanism in "
"the decoder by a a main self-attention mechanism and a self and n-stream "
"(predict) self-attention mechanism."
msgstr ""

#: ../../source/model_summary.rst:735
msgid ""
"The library provides a pre-trained version of this model for conditional "
"generation and a fine-tuned version for summarization."
msgstr ""

#: ../../source/model_summary.rst:739
msgid "XLM-ProphetNet"
msgstr ""

#: ../../source/model_summary.rst:753
msgid ""
"XLM-ProphetNet's model architecture and pretraining objective is same as "
"ProphetNet, but XLM-ProphetNet was pre-trained on the cross-lingual "
"dataset `XGLUE <https://arxiv.org/abs/2004.01401>`__."
msgstr ""

#: ../../source/model_summary.rst:756
msgid ""
"The library provides a pre-trained version of this model for multi-"
"lingual conditional generation and fine-tuned versions for headline "
"generation and question generation, respectively."
msgstr ""

#: ../../source/model_summary.rst:762
msgid "Multimodal models"
msgstr ""

#: ../../source/model_summary.rst:764
msgid ""
"There is one multimodal model in the library which has not been "
"pretrained in the self-supervised fashion like the others."
msgstr ""

#: ../../source/model_summary.rst:768
msgid "MMBT"
msgstr ""

#: ../../source/model_summary.rst:770
msgid ""
"`Supervised Multimodal Bitransformers for Classifying Images and Text "
"<https://arxiv.org/abs/1909.02950>`_, Douwe Kiela et al."
msgstr ""

#: ../../source/model_summary.rst:773
msgid ""
"A transformers model used in multimodal settings, combining a text and an"
" image to make predictions. The transformer model takes as inputs the "
"embeddings of the tokenized text and the final activations of a "
"pretrained on images resnet (after the pooling layer) that goes through a"
" linear layer (to go from number of features at the end of the resnet to "
"the hidden state dimension of the transformer)."
msgstr ""

#: ../../source/model_summary.rst:778
msgid ""
"The different inputs are concatenated, and on top of the positional "
"embeddings, a segment embedding is added to let the model know which part"
" of the input vector corresponds to the text and which to the image."
msgstr ""

#: ../../source/model_summary.rst:781
msgid "The pretrained model only works for classification."
msgstr ""

#: ../../source/model_summary.rst:789
msgid "Retrieval-based models"
msgstr ""

#: ../../source/model_summary.rst:791
msgid ""
"Some models use documents retrieval during (pre)training and inference "
"for open-domain question answering, for example."
msgstr ""

#: ../../source/model_summary.rst:795
msgid "DPR"
msgstr ""

#: ../../source/model_summary.rst:806
msgid ""
"`Dense Passage Retrieval for Open-Domain Question Answering "
"<https://arxiv.org/abs/2004.04906>`_, Vladimir Karpukhin et al."
msgstr ""

#: ../../source/model_summary.rst:809
msgid ""
"Dense Passage Retrieval (DPR) - is a set of tools and models for state-"
"of-the-art open-domain question-answering research."
msgstr ""

#: ../../source/model_summary.rst:813
msgid "DPR consists in three models:"
msgstr ""

#: ../../source/model_summary.rst:815
msgid "Question encoder: encode questions as vectors"
msgstr ""

#: ../../source/model_summary.rst:816
msgid "Context encoder: encode contexts as vectors"
msgstr ""

#: ../../source/model_summary.rst:817
msgid ""
"Reader: extract the answer of the questions inside retrieved contexts, "
"along with a relevance score (high if the inferred span actually answers "
"the question)."
msgstr ""

#: ../../source/model_summary.rst:820
msgid ""
"DPR's pipeline (not implemented yet) uses a retrieval step to find the "
"top k contexts given a certain question, and then it calls the reader "
"with the question and the retrieved documents to get the answer."
msgstr ""

#: ../../source/model_summary.rst:824
msgid "RAG"
msgstr ""

#: ../../source/model_summary.rst:835
msgid ""
"`Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks "
"<https://arxiv.org/abs/2005.11401>`_, Patrick Lewis, Ethan Perez, "
"Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, "
"Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian "
"Riedel, Douwe Kiela"
msgstr ""

#: ../../source/model_summary.rst:839
msgid ""
"Retrieval-augmented generation (\"RAG\") models combine the powers of "
"pretrained dense retrieval (DPR) and Seq2Seq models. RAG models retrieve "
"docs, pass them to a seq2seq model, then marginalize to generate outputs."
" The retriever and seq2seq modules are initialized from pretrained "
"models, and fine-tuned jointly, allowing both retrieval and generation to"
" adapt to downstream tasks."
msgstr ""

#: ../../source/model_summary.rst:844
msgid "The two models RAG-Token and RAG-Sequence are available for generation."
msgstr ""

#: ../../source/model_summary.rst:847
msgid "More technical aspects"
msgstr ""

#: ../../source/model_summary.rst:850
msgid "Full vs sparse attention"
msgstr ""

#: ../../source/model_summary.rst:852
msgid ""
"Most transformer models use full attention in the sense that the "
"attention matrix is square. It can be a big computational bottleneck when"
" you have long texts. Longformer and reformer are models that try to be "
"more efficient and use a sparse version of the attention matrix to speed "
"up training."
msgstr ""

#: ../../source/model_summary.rst:858
msgid "**LSH attention**"
msgstr ""

#: ../../source/model_summary.rst:860
msgid ""
":ref:`Reformer <reformer>` uses LSH attention. In the softmax(QK^t), only"
" the biggest elements (in the softmax dimension) of the matrix QK^t are "
"going to give useful contributions. So for each query q in Q, we can "
"consider only the keys k in K that are close to q. A hash function is "
"used to determine if q and k are close. The attention mask is modified to"
" mask the current token (except at the first position), because it will "
"give a query and a key equal (so very similar to each other). Since the "
"hash can be a bit random, several hash functions are used in practice "
"(determined by a n_rounds parameter) and then are averaged together."
msgstr ""

#: ../../source/model_summary.rst:869
msgid "**Local attention**"
msgstr ""

#: ../../source/model_summary.rst:871
msgid ""
":ref:`Longformer <longformer>` uses local attention: often, the local "
"context (e.g., what are the two tokens to the left and right?) is enough "
"to take action for a given token. Also, by stacking attention layers that"
" have a small window, the last layer will have a receptive field of more "
"than just the tokens in the window, allowing them to build a "
"representation of the whole sentence."
msgstr ""

#: ../../source/model_summary.rst:876
msgid ""
"Some preselected input tokens are also given global attention: for those "
"few tokens, the attention matrix can access all tokens and this process "
"is symmetric: all other tokens have access to those specific tokens (on "
"top of the ones in their local window). This is shown in Figure 2d of the"
" paper, see below for a sample attention mask:"
msgstr ""

#: ../../source/model_summary.rst:884
msgid ""
"Using those attention matrices with less parameters then allows the model"
" to have inputs having a bigger sequence length."
msgstr ""

#: ../../source/model_summary.rst:888
msgid "Other tricks"
msgstr ""

#: ../../source/model_summary.rst:892
msgid "**Axial positional encodings**"
msgstr ""

#: ../../source/model_summary.rst:894
msgid ""
":ref:`Reformer <reformer>` uses axial positional encodings: in "
"traditional transformer models, the positional encoding E is a matrix of "
"size :math:`l` by :math:`d`, :math:`l` being the sequence length and "
":math:`d` the dimension of the hidden state. If you have very long texts,"
" this matrix can be huge and take way too much space on the GPU. To "
"alleviate that, axial positional encodings consist of factorizing that "
"big matrix E in two smaller matrices E1 and E2, with dimensions "
":math:`l_{1} \\times d_{1}` and :math:`l_{2} \\times d_{2}`, such that "
":math:`l_{1} \\times l_{2} = l` and :math:`d_{1} + d_{2} = d` (with the "
"product for the lengths, this ends up being way smaller). The embedding "
"for time step :math:`j` in E is obtained by concatenating the embeddings "
"for timestep :math:`j \\% l1` in E1 and :math:`j // l1` in E2."
msgstr ""

